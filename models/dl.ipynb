{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "wCiT69Br4G0N",
    "ExecuteTime": {
     "end_time": "2024-08-28T13:52:04.339243Z",
     "start_time": "2024-08-28T13:52:04.331633Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "from PIL import Image\n",
    "import os\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ],
   "metadata": {
    "id": "1O4m5dgpiUZi",
    "ExecuteTime": {
     "end_time": "2024-08-28T13:52:07.395456Z",
     "start_time": "2024-08-28T13:52:07.372397Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "#defining generator model\n",
    "class Generator(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Generator,self).__init__()\n",
    "    #calling nn module parameteralized constructor to initialize its internal state\n",
    "    self.model=nn.Sequential(\n",
    "        #creating 2d convulation with 1 input channel and 64 output channels(64 filters)\n",
    "        #1st convulation layer\n",
    "        nn.Conv2d(1,64,kernel_size=3,stride=1,padding=1),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.BatchNorm2d(64),\n",
    "        #2nd convulation layer\n",
    "        nn.Conv2d(64,128,kernel_size=3,stride=1,padding=1),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.BatchNorm2d(128),\n",
    "        #3rd convulation layer\n",
    "        nn.Conv2d(128,512,kernel_size=3,stride=1,padding=1),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.BatchNorm2d(512),\n",
    "        #4th convulation layer\n",
    "        nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.BatchNorm2d(256),\n",
    "        #5th\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.BatchNorm2d(128),\n",
    "        #6th convulation layers\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.BatchNorm2d(64),\n",
    "        #7th convulation layer\n",
    "        nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1),\n",
    "        nn.Tanh()\n",
    "    )\n",
    "  #this generator will produce 2048*2048 3RGBmatrix\n",
    "  def forward(self,x):\n",
    "    return self.model(x)"
   ],
   "metadata": {
    "id": "nXQ9P6zii1lm",
    "ExecuteTime": {
     "end_time": "2024-08-28T13:52:08.211858Z",
     "start_time": "2024-08-28T13:52:08.188204Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "class Discriminator(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Discriminator,self).__init__()\n",
    "    self.model=nn.Sequential(\n",
    "      #1 conv layer\n",
    "      nn.Conv2d(3,64,kernel_size=3,stride=2,padding=1),#2048 to 1024\n",
    "      nn.LeakyReLU(0.2),\n",
    "      nn.Dropout(0.25),\n",
    "      #randomly drop 25% neurons to avoid overfitting\n",
    "      nn.Conv2d(64,128,kernel_size=3,stride=2,padding=1),#1024 to 512\n",
    "      nn.BatchNorm2d(128),\n",
    "      nn.Dropout(0.25),\n",
    "\n",
    "      nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),#512 to 256\n",
    "      nn.LeakyReLU(0.2),\n",
    "       nn.BatchNorm2d(256),\n",
    "      nn.Dropout(0.25),\n",
    "\n",
    "     nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),#256 to 256\n",
    "     nn.LeakyReLU(0.2),\n",
    "     nn.BatchNorm2d(512),\n",
    "     nn.Dropout(0.25),\n",
    "\n",
    "     nn.Flatten(),\n",
    "     nn.Linear(524288,1),\n",
    "     nn.Sigmoid()\n",
    "\n",
    "      #flattening to convert the multidimensional tensor\n",
    "      #to single dimensional tensor so that it can be taken as input for classification task done by neural descriminator\n",
    "\n",
    "\n",
    "\n",
    "    )\n",
    "  def forward(self,x):\n",
    "    return self.model(x)"
   ],
   "metadata": {
    "id": "Izg6qXvL_nS-",
    "ExecuteTime": {
     "end_time": "2024-08-28T13:56:49.293140Z",
     "start_time": "2024-08-28T13:56:49.270103Z"
    }
   },
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": [
    "#defining loss functions and optimizers\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "def get_optimizer(model, lr=0.0002, beta1=0.5):\n",
    "    return optim.Adam(model.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "#loss function is Binary cross entropy loss and optimizer is Adam\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    transform = transforms.ToTensor()  # Converts image to PyTorch tensor\n",
    "    return transform(image)\n",
    "\n",
    "def load_data(sar_dir, optical_dir):\n",
    "    sar_images = []\n",
    "    optical_images = []\n",
    "\n",
    "\n",
    "    sar_files = sorted(os.listdir(sar_dir))\n",
    "    optical_files = sorted(os.listdir(optical_dir))\n",
    "\n",
    "    for filename1, filename2 in zip(sar_files, optical_files):\n",
    "        sar_image = load_image(os.path.join(sar_dir, filename1))\n",
    "        optical_image = load_image(os.path.join(optical_dir, filename2))\n",
    "        sar_images.append(sar_image[:, 0:1])  # Keep only the first channel (Grayscale)\n",
    "        optical_images.append(optical_image)\n",
    "\n",
    "    return torch.stack(sar_images), torch.stack(optical_images)  # Stack to form a batch\n",
    "\n",
    "def normalize(tensor):\n",
    "    return (tensor - 0.5) * 2\n",
    "\n",
    "def normalize_images(sar_dir, optical_dir):\n",
    "    sar_images_tensor, optical_images_tensor = load_data(sar_dir, optical_dir)\n",
    "\n",
    "    normalized_sar_images = normalize(sar_images_tensor)\n",
    "    normalized_optical_images = normalize(optical_images_tensor)\n",
    "\n",
    "    return normalized_sar_images, normalized_optical_images\n",
    "from sklearn.model_selection import train_test_split\n",
    "def train_test_split_data(sar_images, optical_images, test_size=0.2):\n",
    "    # Flatten the list of images into 1D array for splitting\n",
    "    indices = list(range(len(sar_images)))\n",
    "    train_idx, test_idx = train_test_split(indices, test_size=test_size, random_state=42)\n",
    "\n",
    "    sar_train = sar_images[train_idx]\n",
    "    optical_train = optical_images[train_idx]\n",
    "\n",
    "    sar_test = sar_images[test_idx]\n",
    "    optical_test = optical_images[test_idx]\n",
    "\n",
    "    return sar_train, optical_train, sar_test, optical_test\n",
    "\n"
   ],
   "metadata": {
    "id": "BzFwCamxUi_w",
    "ExecuteTime": {
     "end_time": "2024-08-28T13:52:13.793514Z",
     "start_time": "2024-08-28T13:52:11.682032Z"
    }
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T13:53:47.329188Z",
     "start_time": "2024-08-28T13:52:16.902012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sar_dir = \"C:/Users/sujal/PycharmProjects/scientificProject/sar_image\"\n",
    "optical_dir = r\"C:/Users/sujal/PycharmProjects/scientificProject/optical_image\"\n",
    "normalized_sar_images, normalized_optical_images = normalize_images(sar_dir, optical_dir)"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T13:53:54.474103Z",
     "start_time": "2024-08-28T13:53:54.389943Z"
    }
   },
   "cell_type": "code",
   "source": "normalized_optical_images,normalized_sar_images\n",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 0.1765, -0.0510, -0.3020,  ..., -0.3255, -0.3961, -0.4196],\n",
       "           [ 0.1529,  0.1686, -0.1137,  ..., -0.1765, -0.2941, -0.4353],\n",
       "           [ 0.1059,  0.0902, -0.0745,  ..., -0.3569, -0.2392, -0.2941],\n",
       "           ...,\n",
       "           [-0.3882, -0.3412, -0.4510,  ...,  0.8118,  0.5686,  1.0000],\n",
       "           [-0.3961,  0.2784, -0.3961,  ...,  0.5216,  0.3804,  0.4745],\n",
       "           [ 0.2784,  0.8118, -0.0196,  ..., -0.0196, -0.0196, -0.0039]],\n",
       " \n",
       "          [[ 0.0510, -0.0275, -0.2235,  ..., -0.3412, -0.3882, -0.4824],\n",
       "           [ 0.0588,  0.0745, -0.0510,  ..., -0.3412, -0.4196, -0.4824],\n",
       "           [-0.0275, -0.0510, -0.0980,  ..., -0.2235, -0.2706, -0.3882],\n",
       "           ...,\n",
       "           [-0.3412, -0.2000, -0.3412,  ...,  0.7804,  0.2627,  1.0000],\n",
       "           [-0.1451,  0.5059, -0.2863,  ...,  0.4275,  0.2078,  0.3020],\n",
       "           [ 0.5529,  0.8588, -0.0510,  ..., -0.0588, -0.0824, -0.0667]],\n",
       " \n",
       "          [[-0.1608, -0.2863, -0.5059,  ..., -0.5216, -0.5765, -0.5922],\n",
       "           [-0.0980, -0.0902, -0.2784,  ..., -0.4667, -0.5529, -0.6314],\n",
       "           [-0.1922, -0.1922, -0.2078,  ..., -0.4667, -0.4353, -0.5216],\n",
       "           ...,\n",
       "           [-0.5294, -0.4588, -0.5765,  ...,  0.7255,  0.2157,  1.0000],\n",
       "           [-0.3961,  0.5451, -0.4118,  ...,  0.3176,  0.1216,  0.2863],\n",
       "           [ 0.1765,  0.7569, -0.1922,  ..., -0.0745, -0.0510, -0.1137]]],\n",
       " \n",
       " \n",
       "         [[[-0.8431, -0.8510, -0.8118,  ..., -0.5294, -0.6314, -0.7098],\n",
       "           [-0.8667, -0.8510, -0.8353,  ..., -0.4275, -0.5137, -0.6157],\n",
       "           [-0.9059, -0.8824, -0.8510,  ..., -0.4588, -0.4667, -0.4745],\n",
       "           ...,\n",
       "           [-0.6157, -0.6314, -0.7412,  ..., -0.4588, -0.4510, -0.5216],\n",
       "           [-0.4588, -0.6784, -0.7255,  ..., -0.3255, -0.3098, -0.3961],\n",
       "           [-0.5137, -0.7176, -0.7333,  ..., -0.2235, -0.2314, -0.3255]],\n",
       " \n",
       "          [[-0.7020, -0.6863, -0.5529,  ..., -0.4745, -0.5451, -0.6157],\n",
       "           [-0.7255, -0.7098, -0.6784,  ..., -0.4431, -0.4902, -0.5451],\n",
       "           [-0.7490, -0.7333, -0.6863,  ..., -0.4275, -0.4275, -0.4667],\n",
       "           ...,\n",
       "           [-0.5059, -0.5137, -0.5843,  ..., -0.4745, -0.4510, -0.5294],\n",
       "           [-0.3961, -0.5216, -0.5529,  ..., -0.3882, -0.4118, -0.4431],\n",
       "           [-0.4353, -0.5529, -0.4588,  ..., -0.2627, -0.3176, -0.3882]],\n",
       " \n",
       "          [[-0.8275, -0.8118, -0.7725,  ..., -0.6392, -0.7176, -0.7412],\n",
       "           [-0.8431, -0.8196, -0.8353,  ..., -0.5843, -0.6784, -0.6941],\n",
       "           [-0.8510, -0.8510, -0.8118,  ..., -0.6078, -0.5843, -0.5686],\n",
       "           ...,\n",
       "           [-0.7098, -0.7569, -0.7725,  ..., -0.4902, -0.4588, -0.5137],\n",
       "           [-0.6471, -0.7725, -0.7961,  ..., -0.5686, -0.5686, -0.5922],\n",
       "           [-0.6706, -0.7725, -0.7255,  ..., -0.3569, -0.3647, -0.4588]]],\n",
       " \n",
       " \n",
       "         [[[ 1.0000,  1.0000,  0.3098,  ..., -0.0980, -0.0275,  0.1216],\n",
       "           [ 1.0000,  1.0000,  0.6235,  ...,  0.1922, -0.0039,  0.3961],\n",
       "           [ 1.0000,  1.0000,  0.2863,  ...,  0.6392, -0.0196,  0.0745],\n",
       "           ...,\n",
       "           [-0.8588, -0.8588, -0.8667,  ..., -0.0667,  0.4980,  0.2000],\n",
       "           [-0.8588, -0.8667, -0.8667,  ..., -0.2314, -0.2471, -0.2078],\n",
       "           [-0.8431, -0.8588, -0.8588,  ..., -0.2000, -0.2784, -0.5294]],\n",
       " \n",
       "          [[ 1.0000,  1.0000,  0.3098,  ..., -0.2314, -0.0667,  0.0353],\n",
       "           [ 1.0000,  1.0000,  0.6863,  ...,  0.1686,  0.0431,  0.2941],\n",
       "           [ 1.0000,  0.9373,  0.3255,  ...,  0.6471,  0.0196,  0.0275],\n",
       "           ...,\n",
       "           [-0.6157, -0.5059, -0.5059,  ..., -0.0902,  0.4118,  0.1922],\n",
       "           [-0.6157, -0.5294, -0.5608,  ..., -0.2157, -0.1686, -0.1529],\n",
       "           [-0.5216, -0.5608, -0.5608,  ..., -0.1608, -0.1608, -0.4039]],\n",
       " \n",
       "          [[ 1.0000,  1.0000,  0.2078,  ..., -0.2235, -0.0588,  0.0196],\n",
       "           [ 1.0000,  1.0000,  0.4824,  ...,  0.1765,  0.0353,  0.1529],\n",
       "           [ 1.0000,  0.9765,  0.1686,  ...,  0.5216,  0.0039,  0.0431],\n",
       "           ...,\n",
       "           [-0.8431, -0.8275, -0.8667,  ..., -0.2627,  0.2000,  0.0196],\n",
       "           [-0.8667, -0.8431, -0.8510,  ..., -0.2863, -0.2392, -0.2863],\n",
       "           [-0.8196, -0.8353, -0.8353,  ..., -0.2157, -0.3804, -0.6157]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 1.0000,  1.0000,  1.0000,  ..., -0.5922, -0.7412, -0.7647],\n",
       "           [ 1.0000,  1.0000,  1.0000,  ..., -0.8431, -0.8118, -0.8275],\n",
       "           [ 1.0000,  1.0000,  1.0000,  ..., -0.8824, -0.8196, -0.8275],\n",
       "           ...,\n",
       "           [-0.8745, -0.9059, -0.8902,  ..., -0.9216, -0.9216, -0.9216],\n",
       "           [-0.8902, -0.8824, -0.8353,  ..., -0.8980, -0.8980, -0.8902],\n",
       "           [-0.7804, -0.7961, -0.8039,  ..., -0.9059, -0.9059, -0.8980]],\n",
       " \n",
       "          [[ 1.0000,  1.0000,  1.0000,  ..., -0.5686, -0.6784, -0.7020],\n",
       "           [ 1.0000,  1.0000,  1.0000,  ..., -0.7647, -0.7725, -0.7647],\n",
       "           [ 1.0000,  1.0000,  1.0000,  ..., -0.8196, -0.7412, -0.7333],\n",
       "           ...,\n",
       "           [-0.8745, -0.9216, -0.8431,  ..., -0.9216, -0.9216, -0.8745],\n",
       "           [-0.8588, -0.8275, -0.7882,  ..., -0.8902, -0.8902, -0.8196],\n",
       "           [-0.6706, -0.7255, -0.7490,  ..., -0.8824, -0.8824, -0.8510]],\n",
       " \n",
       "          [[ 1.0000,  1.0000,  1.0000,  ..., -0.7412, -0.8039, -0.7961],\n",
       "           [ 1.0000,  1.0000,  0.8824,  ..., -0.9059, -0.8980, -0.8824],\n",
       "           [ 1.0000,  1.0000,  0.7490,  ..., -0.8745, -0.8353, -0.8510],\n",
       "           ...,\n",
       "           [-0.9059, -0.9843, -0.9137,  ..., -0.9373, -0.9373, -0.9373],\n",
       "           [-0.9451, -0.8902, -0.8902,  ..., -0.9216, -0.9216, -0.9216],\n",
       "           [-0.8118, -0.8353, -0.8353,  ..., -0.9216, -0.9216, -0.9294]]],\n",
       " \n",
       " \n",
       "         [[[-0.9059, -0.8980, -0.8824,  ..., -0.7804, -0.7882, -0.7725],\n",
       "           [-0.8980, -0.8902, -0.8824,  ..., -0.7490, -0.7647, -0.7490],\n",
       "           [-0.8824, -0.8824, -0.8902,  ..., -0.7490, -0.7647, -0.7490],\n",
       "           ...,\n",
       "           [-0.5373, -0.4353, -0.6863,  ...,  0.0588,  0.0588, -0.0039],\n",
       "           [-0.2314, -0.3804, -0.6863,  ...,  0.1059,  0.1059,  0.0196],\n",
       "           [-0.0353, -0.3412, -0.8039,  ...,  0.1608,  0.0196,  0.0196]],\n",
       " \n",
       "          [[-0.8824, -0.8745, -0.8353,  ..., -0.7412, -0.7176, -0.7176],\n",
       "           [-0.8745, -0.8824, -0.8431,  ..., -0.7098, -0.7098, -0.7569],\n",
       "           [-0.8745, -0.8902, -0.8510,  ..., -0.7412, -0.7569, -0.7490],\n",
       "           ...,\n",
       "           [-0.5059, -0.2784, -0.6549,  ...,  0.0431,  0.0431,  0.0118],\n",
       "           [-0.2706, -0.3725, -0.6549,  ..., -0.0745, -0.0745,  0.0588],\n",
       "           [ 0.0039, -0.2941, -0.8353,  ...,  0.0902,  0.0353,  0.0353]],\n",
       " \n",
       "          [[-0.9451, -0.9451, -0.9059,  ..., -0.8510, -0.8275, -0.8353],\n",
       "           [-0.9373, -0.9373, -0.8902,  ..., -0.8196, -0.8118, -0.8118],\n",
       "           [-0.9059, -0.9216, -0.9059,  ..., -0.7882, -0.8039, -0.8196],\n",
       "           ...,\n",
       "           [-0.5922, -0.4510, -0.7098,  ...,  0.1843,  0.1843,  0.0510],\n",
       "           [-0.2706, -0.5843, -0.7098,  ..., -0.0039, -0.0039,  0.1686],\n",
       "           [-0.0353, -0.5137, -0.8431,  ...,  0.2000,  0.0824,  0.0824]]],\n",
       " \n",
       " \n",
       "         [[[-0.7255, -0.8118, -0.8196,  ..., -0.9137, -0.9137, -0.9059],\n",
       "           [-0.7961, -0.8745, -0.8510,  ..., -0.8824, -0.8824, -0.8902],\n",
       "           [-0.8431, -0.8824, -0.8510,  ..., -0.8902, -0.8902, -0.8902],\n",
       "           ...,\n",
       "           [ 0.2314,  0.1294, -0.0902,  ..., -0.9373, -0.9216, -0.9216],\n",
       "           [ 0.1843,  0.1451, -0.1373,  ..., -0.9373, -0.9216, -0.9216],\n",
       "           [-0.0118,  0.1451,  0.1059,  ..., -0.9216, -0.9294, -0.9294]],\n",
       " \n",
       "          [[-0.6627, -0.7490, -0.7725,  ..., -0.8745, -0.8745, -0.8510],\n",
       "           [-0.7490, -0.8118, -0.8824,  ..., -0.8588, -0.8588, -0.8510],\n",
       "           [-0.7961, -0.8588, -0.8118,  ..., -0.8667, -0.8667, -0.8510],\n",
       "           ...,\n",
       "           [ 0.1765,  0.0902, -0.0980,  ..., -0.9059, -0.8588, -0.8588],\n",
       "           [ 0.1373,  0.0588, -0.1686,  ..., -0.8824, -0.8745, -0.8745],\n",
       "           [ 0.0196,  0.1843,  0.1137,  ..., -0.8824, -0.8588, -0.8588]],\n",
       " \n",
       "          [[-0.7725, -0.8118, -0.8745,  ..., -0.9216, -0.9216, -0.8980],\n",
       "           [-0.8510, -0.8824, -0.8902,  ..., -0.8980, -0.8980, -0.8980],\n",
       "           [-0.8667, -0.8588, -0.8588,  ..., -0.9059, -0.9059, -0.9373],\n",
       "           ...,\n",
       "           [ 0.2549,  0.1686, -0.1843,  ..., -0.9451, -0.9451, -0.9451],\n",
       "           [ 0.2471,  0.1373, -0.1529,  ..., -0.9294, -0.9294, -0.9294],\n",
       "           [ 0.1294,  0.2078,  0.1843,  ..., -0.9216, -0.9137, -0.9137]]]]),\n",
       " tensor([[[[ 0.1765,  0.0275, -0.0588,  ...,  0.3412,  0.3412,  0.3176]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1608,  0.2000,  0.2000,  ...,  0.0980,  0.0824,  0.1216]]],\n",
       " \n",
       " \n",
       "         [[[ 0.8118,  0.8118,  1.0000,  ...,  0.3804,  0.4980,  0.5686]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 0.4275, -0.2078, -0.1922,  ...,  0.8353,  0.7020,  0.4196]]],\n",
       " \n",
       " \n",
       "         [[[ 0.5686,  0.5686,  0.5373,  ...,  0.5529,  0.4824,  0.4196]]],\n",
       " \n",
       " \n",
       "         [[[ 0.4902,  0.4902,  0.4824,  ...,  0.0196,  0.2784,  0.5059]]]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T13:54:05.464315Z",
     "start_time": "2024-08-28T13:54:05.433155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_gan(generator, discriminator, gan, sar_train, optical_train, sar_test, optical_test, epochs=10000, batch_size=32):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    generator.to(device)\n",
    "    discriminator.to(device)\n",
    "    gan.to(device)\n",
    "\n",
    "    valid = torch.ones((batch_size, 1), device=device)\n",
    "    fake = torch.zeros((batch_size, 1), device=device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        idx = torch.randint(0, sar_train.size(0), (batch_size,))\n",
    "        sar_batch = sar_train[idx].to(device)\n",
    "        optical_batch = optical_train[idx].to(device)\n",
    "\n",
    "        # Training discriminator\n",
    "        discriminator.zero_grad()\n",
    "        real_loss = criterion(discriminator(optical_batch), valid)\n",
    "        fake_images = generator(sar_batch).detach()\n",
    "        fake_loss = criterion(discriminator(fake_images), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        d_loss.backward()\n",
    "        optimizer_d.step()\n",
    "        \n",
    "        #training the generator\n",
    "        generator.zero_grad()\n",
    "        g_loss = criterion(discriminator(generator(sar_batch)), valid)\n",
    "        g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"{epoch} [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\")\n",
    "            sample_images(generator, sar_batch, epoch, sar_test)\n",
    "            \n",
    "def sample_images(generator, sar_images, epoch, sar_test):\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        generated_images = generator(sar_images).cpu().numpy()\n",
    "        test_images = generator(sar_test).cpu().numpy()\n",
    "\n",
    "    r, c = 2, 3\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    for i in range(c):\n",
    "        axs[0, i].imshow(sar_images[i].cpu().squeeze(0).squeeze(0), cmap='gray')\n",
    "        axs[1, i].imshow((generated_images[i].transpose(1, 2, 0) + 1) / 2)  # Rescale [-1, 1] to [0, 1]\n",
    "        axs[1, c + i].imshow((test_images[i].transpose(1, 2, 0) + 1) / 2)  # Rescale [-1, 1] to [0, 1]\n",
    "    plt.show()\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T13:54:09.592147Z",
     "start_time": "2024-08-28T13:54:09.576647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GAN(nn.Module):\n",
    "    def __init__(self, generator, discriminator):\n",
    "        super(GAN, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "\n",
    "    def forward(self, z):\n",
    "        # Generate images from the generator\n",
    "        generated_images = self.generator(z)\n",
    "        # Classify the generated images with the discriminator\n",
    "        validity = self.discriminator(generated_images)\n",
    "        return validity"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T13:57:10.723640Z",
     "start_time": "2024-08-28T13:56:56.201498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "gan = GAN(generator, discriminator)\n",
    "\n",
    "optimizer_g = get_optimizer(generator)\n",
    "optimizer_d = get_optimizer(discriminator)\n",
    "sar_train, optical_train, sar_test, optical_test = train_test_split_data(normalized_sar_images,normalized_optical_images, test_size=0.2)\n",
    "\n",
    "\n",
    "train_gan(generator, discriminator, gan, sar_train, optical_train, sar_test, optical_test, epochs=10000, batch_size=32)\n"
   ],
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x131072 and 524288x1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 10\u001B[0m\n\u001B[0;32m      6\u001B[0m optimizer_d \u001B[38;5;241m=\u001B[39m get_optimizer(discriminator)\n\u001B[0;32m      7\u001B[0m sar_train, optical_train, sar_test, optical_test \u001B[38;5;241m=\u001B[39m train_test_split_data(normalized_sar_images,normalized_optical_images, test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m)\n\u001B[1;32m---> 10\u001B[0m train_gan(generator, discriminator, gan, sar_train, optical_train, sar_test, optical_test, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10000\u001B[39m, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m)\n",
      "Cell \u001B[1;32mIn[14], line 19\u001B[0m, in \u001B[0;36mtrain_gan\u001B[1;34m(generator, discriminator, gan, sar_train, optical_train, sar_test, optical_test, epochs, batch_size)\u001B[0m\n\u001B[0;32m     17\u001B[0m real_loss \u001B[38;5;241m=\u001B[39m criterion(discriminator(optical_batch), valid)\n\u001B[0;32m     18\u001B[0m fake_images \u001B[38;5;241m=\u001B[39m generator(sar_batch)\u001B[38;5;241m.\u001B[39mdetach()\n\u001B[1;32m---> 19\u001B[0m fake_loss \u001B[38;5;241m=\u001B[39m criterion(discriminator(fake_images), fake)\n\u001B[0;32m     20\u001B[0m d_loss \u001B[38;5;241m=\u001B[39m (real_loss \u001B[38;5;241m+\u001B[39m fake_loss) \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[0;32m     21\u001B[0m d_loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[17], line 35\u001B[0m, in \u001B[0;36mDiscriminator.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m,x):\n\u001B[1;32m---> 35\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(x)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    217\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 219\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m module(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m    220\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mlinear(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (32x131072 and 524288x1)"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
